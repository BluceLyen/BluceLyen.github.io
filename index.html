<!DOCTYPE HTML>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Bluce Lyen</title>
  <meta name="author" content="Lyen">
  
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  
  <meta property="og:site_name" content="Bluce Lyen"/>

  
    <meta property="og:image" content="undefined"/>
  

  <link href="/favicon.ico" rel="icon">
  <link rel="alternate" href="/atom.xml" title="Bluce Lyen" type="application/atom+xml">
  <link rel="stylesheet" href="//netdna.bootstrapcdn.com/bootstrap/3.1.0/css/bootstrap.min.css" type="text/css">
<link rel="stylesheet" href="//netdna.bootstrapcdn.com/font-awesome/4.0.3/css/font-awesome.css" type="text/css">
<link rel="stylesheet" href="/css/style.css" type="text/css">
  <script src="http://code.jquery.com/jquery-2.1.1.min.js"></script>
  <!--[if lt IE 9]><script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]-->
  
<script type="text/javascript">
  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-368771XX-X']);
  _gaq.push(['_trackPageview']);

  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();
</script>

</head>

<body>
  <header id="header" class='normal_mode'>
    <nav id="main-nav">
  <ul class='container'>
    
      <li><a href="/">Home</a></li>
    
      <li><a href="/archives">Archives</a></li>
    
  </ul>
  <div class="clearfix"></div>
</nav>
  </header>
  <div id="content" class="container">
    <div id="cover">
	<div id="profile" alt="http://www.33lc.com/article/UploadPic/2012-7/20127291041962908.jpg">
		<a href="/">
			<div class="logo">
				<img src="/logo.png" alt="Profile Picture">
			</div>
			<div id="title">Bluce Lyen</div>
		</a>

		
		 <ul class="my-socials">
  
  <li>
  	<a href="https://github.com/BluceLyen" class="github" target="_blank">
  		<i class="fa fa-github"></i>
  	</a>
  </li>
  
  <li>
  	<a href="http://weibo.com/u/2530759420/home?topnav=1&wvr=6" class="weibo" target="_blank">
  		<i class="fa fa-weibo"></i>
  	</a>
  </li>
  
 
</ul>
	</div>
</div>


  <article class="post">
  
    <div class="gallery">
  <div class="photoset">
    
      <img src="">
    
  </div>
  <div class="control">
    <div class="prev"></div>
    <div class="next"></div>
  </div>
</div>
  
  <div class="post-content">
    <header>
      
      <time datetime="2016-03-30T06:59:44.364Z"><a href="/2016/03/30/IDC/">Mié., Mar. 30 2016, 2:59:44 pm</a></time>

  
    <h1 class="title"><a href="/2016/03/30/IDC/">IDC用户网站访问记录系统设计与开发</a></h1>
  



<div class="clear"></div>
      
    </header>
    <div class="entry">
      
        <p>##1.设计思路</p>
<p>###1.1问题分析<br>IDC用户网站访问记录系统设计与开发，顾名思义，本<br>系统是为了分析用户某段时间内浏览网页的行为，我们需要计算哪些用户访问了哪些网页，对他们的行为进行分析，反映出网站的的热度等，程序设计思路理所应当是高效写入和查询数据。</p>
<p>###1.2设计目标</p>
<p>####1.2.1 模拟数据写入模块<br>在写入数据部分应该实现一个实时机制，在写入数据这一模块中,从数据在客服端产生到上传到hdfs再到将数据写入到Hbase这个过程涉及三个程序,第一个程序每隔一分钟生成模拟数据文件放在本地的一个文件夹中,第二个程序负责将生成的数据文件实时的上传到hdfs,第三个程序负责把hdfs上边的文件实时的写入Hbase中,由于考虑到实时,这三个程序构成一个实时的机制,数据产生即刻上传至hdfs并且即刻通过mapreduce直接写入Hbase。</p>
<p>####1.2.2 数据查询模块<br>查询部分本系统满足按时间段查询，按时间范围+ip范围查询完整记录，按时间范围+ip范围+协议类型查询完整记录，按时间范围+ip范围+协议类型+url域名查询完整记录。</p>
<p>###1.3 研究思路</p>
<p>####1.3.1怎样写入Hbase比较科学?<br>数据模拟程序每分钟生成一个数据文件，当数据还未生成成功时后缀名为.lock，因此我们需要利用File文件进行遍历目录，将完整文件上传至hdfs指定的变量目录，每上传一个或者多个文件，同时删除本地的文件与此同时执行mapreduce将文件数据写入Hbase，写完这个文件则从hdfs中删除，这样可以保证mapreduce在写程序的时候在这个指定目录下不会mapreduce重复的文件。</p>
<p>####1.3.2如何设计rowkey？<br>rowkey每条数据都会对时间进行检索，所以将时间戳放到了timestamp的高位，后接上用户的ip，保证rowkey的唯一性，二来方便检索用户。因为时间连续性的timestamp会造成单region热点问题，理应进行hash散列或者region分区，但由于只有一台单节点服务器，因此就没有做过多的分区处理，这样的rowkey检索性能也相对来说很高。</p>
<p>####1.3.3如何进行查询？<br>a.Scan和Filter的方法，当有多种过滤条件的时候使用FilterList过滤链。</p>
<p>b.Coprocessor方法。</p>
<p>注:由于目前所学知识较少,查询用的是第一种方法.</p>
<p>##2.系统实现</p>
<p>###2.1 写入模块</p>
<p>####2.1.1写入数据<br>连续写入数据20分钟，检查表里的记录是否为2880000条。</p>
<p>####2.1.2核心代码<br>    public static void main(String[] args) throws IOException, InterruptedException {<br>                /<em>
                 </em>生成数据<br>                 <em>/<br>        String path = “/home/lyen/data/“;<br>        int i = 0;<br>        while(i&lt;20){<br>            String filename = String.valueOf(System.currentTimeMillis());<br>            FileWriter fw = new FileWriter(path+filename+”.lock”);<br>            String result = getResult();<br>            fw.write(result);<br>            fw.close();<br>            File file = new File(path+filename+”.lock”);<br>            file.renameTo(new File(path+filename));<br>            i++;<br>             System.out.println(i);<br>            Thread.sleep(60000);<br>        }<br>    }<br>    public static void main(String[] args) throws IOException, InterruptedException {<br>        /</em></p>
<pre><code>     <span class="built_in">*</span> 上传文件到hdfs,文件上传完以后删除本地文件
     <span class="built_in">*</span>/
    String hdfsPath = <span class="string">"hdfs://master:9000/experiment"</span>;
    createDir<span class="params">(hdfsPath)</span>;
    String localPath = <span class="string">"/home/lyen/data/"</span>;
    File file = new File<span class="params">(localPath)</span>;
    int fileNumber = <span class="number">0</span>;
    <span class="keyword">if</span><span class="params">(file.isDirectory<span class="params">()</span>)</span>{
        while<span class="params">(fileNumber&lt;<span class="number">20</span>)</span>{
            File[] files = file.listFiles<span class="params">()</span>;
            <span class="keyword">if</span><span class="params">(files.length!=<span class="number">0</span>)</span>{
            <span class="keyword">for</span><span class="params">(int i=<span class="number">0</span>;i&lt;files.length;i++)</span>{
                <span class="keyword">if</span><span class="params">(!files[i].getName<span class="params">()</span>.endsWith<span class="params">(<span class="string">".lock"</span>)</span>)</span>{
                    File upfile = new File<span class="params">(localPath+files[i].getName<span class="params">()</span>)</span>;
                    copyFile<span class="params">(upfile.getAbsolutePath<span class="params">()</span>, hdfsPath)</span>;
                    fileNumber+=files.length;
                    <span class="keyword">if</span><span class="params">(fs.exists<span class="params">(new Path<span class="params">(hdfsPath+<span class="string">"/"</span>+ upfile.getName<span class="params">()</span>)</span>)</span>)</span>{
                        upfile.delete<span class="params">()</span>;
                    }
                }
            }
            }<span class="keyword">else</span>{
                System.out.println<span class="params">(<span class="string">"文件还没有获取,请耐心等待"</span>)</span>;
            }
        }
    }
    System.out.println<span class="params">(<span class="string">"上传文件总数:"</span>+fileNumber)</span>;

}

public static void main<span class="params">(String[] args)</span> throws IOException, Exception {
    <span class="comment">/*
     * 写数据到hbase
     */</span>
    int fileNum=<span class="number">0</span>;
    Long begintime = System.currentTimeMillis<span class="params">()</span>;
    Date date = new Date<span class="params">(begintime)</span>;
    while <span class="params">(fileNum&lt;<span class="number">20</span>)</span> {
        FileStatus[] fileStatus = fs.listStatus<span class="params">(HDFS_PATH)</span>;
        <span class="keyword">if</span> <span class="params">(fileStatus.length !=<span class="number">0</span>)</span> {
            runWrite<span class="params">(HDFS_PATH)</span>;
            fileNum+=fileStatus.length;
            <span class="keyword">for</span> <span class="params">(int i = <span class="number">0</span>; i &lt; fileStatus.length; i++)</span> {
                <span class="keyword">if</span><span class="params">( fs.exists<span class="params">(fileStatus[i].getPath<span class="params">()</span>)</span>)</span>{
                    Table_util.deleteFile<span class="params">(fileStatus[i].getPath<span class="params">()</span>)</span>;
                    System.out.println<span class="params">(<span class="string">"文件已删除!"</span>)</span>;
                }
            }
        } <span class="keyword">else</span> {
            System.out.println<span class="params">(<span class="string">"文件正在上传或者数据已经完全写入Hbase"</span>)</span>;
        }
    }
    SimpleDateFormat sdf=new SimpleDateFormat<span class="params">(<span class="string">"yyyy-MM-dd hh:mm:ss"</span>)</span>;  
    String sd = sdf.format<span class="params">(date)</span>;
    System.out.println<span class="params">(<span class="string">"写入开始时间:"</span>+sd)</span>;
    System.out.println<span class="params">(<span class="string">"写入Hbase文件数:"</span>+fileNum)</span>;
    Long stoptime = System.currentTimeMillis<span class="params">()</span>;
    SimpleDateFormat sdf1=new SimpleDateFormat<span class="params">(<span class="string">"yyyy-MM-dd hh:mm:ss"</span>)</span>;  
    String s = sdf1.format<span class="params">(new Date<span class="params">(stoptime)</span>)</span>;
    System.out.println<span class="params">(<span class="string">"写入完成时间"</span>+s)</span>;
    Long costtime = stoptime-begintime;
    <span class="built_in">log</span>.info<span class="params">(<span class="string">"文件从客服端接收上传到hdfs再写入Hbase花费时间:"</span>+costtime+<span class="string">"ms"</span>)</span>;
    System.out.println<span class="params">(<span class="string">"文件从接收到hdfs再写入Hbase花费时间:"</span>+new Date<span class="params">(costtime)</span>.getMinutes<span class="params">()</span>+<span class="string">"min"</span>)</span>;

}
</code></pre><p>####2.1.3程序效果图</p>
<p><img src="/IDC/01.png" alt=""></p>
<p>###2.2查询模块</p>
<p>####2.2.1查询数据条件<br>1.统计记录全部条数。</p>
<p>2.查询写入时间开始起的第2:00-3:00这一分钟内协议类型为HTTPS的记录条数。</p>
<p>3.查询写入时间开始起的第16:00-18:00这两分钟内，ip范围为[40.1.1.1-49.1.1.225],协议类型为HTTP,URL为google.com的记录条数。</p>
<p>####2.2.2核心代码<br>    //1.查询所有记录数目<br>    public static void count() throws IOException {<br>        Long startTime = System.currentTimeMillis();<br>        Scan scan = new Scan();<br>        scan.setCaching(100000)//设置缓存<br>        scan.setCacheBlocks(false);<br>        scan.setFilter(new FirstKeyOnlyFilter());<br>        ResultScanner rs = table.getScanner(scan);<br>        int num = 0;<br>        for (Result r : rs) {<br>            num++;<br>        }<br>        System.out.println(“数据记录:”+num);<br>        Long endTime = System.currentTimeMillis();<br>        Long time = endTime - startTime;<br>        log.info(“cost time” + “—–&gt;” + time + “ms”);<br>    }<br>    //2.根据时间,协议类型查询所有符合的记录数目<br>    public static void countLimitedTT(Long stime ,int b,int s,String netType) throws IOException {<br>        Long bTime = System.currentTimeMillis();<br>        stime = startTime;<br>        String tmp001 = util(stime, b);<br>        String tmp002 = util(stime, s);<br>        FilterList filterList = new FilterList();<br>        filterList.addFilter(new FamilyFilter(CompareFilter.CompareOp.EQUAL, new BinaryComparator(“data”.getBytes())));<br>        filterList.addFilter(new ValueFilter(CompareFilter.CompareOp.EQUAL, new SubstringComparator(netType)));<br>        Scan scan = new Scan(tmp001.getBytes() ,tmp002.getBytes());<br>        scan.setCaching(10000);<br>        scan.setFilter(filterList);<br>        ResultScanner rs = table.getScanner(scan);<br>        int num = 0;<br>       for(Result r:rs){<br>           num++;<br>           for  (Cell cell : r.rawCells()) {<br>               System. out .println(<br>                        “Rowkey : “ +Bytes.toString(r.getRow()).split(“-“)[0]+”-“+     Convert.bytesToIpByReg(Bytes.copy(cell.getRowArray(),cell.getRowOffset()+cell.getRowLength()-4,8))+<br>                         “ family:” +Bytes.toString(cell.getFamily())+<br>                         “   Familiy:Quilifier : “ +Bytes. toString (CellUtil. cloneQualifier (cell))+<br>                         “   Value : “ +Bytes. toString (CellUtil. cloneValue (cell))+<br>                        “Time : “ +cell.getTimestamp()<br>                        );<br>            }<br>       }<br>    System.out.println(“数据生成2-3分钟,协议类型为HTTPS的记录:”+num);<br>       Long sTime = System.currentTimeMillis();<br>       Long time = sTime-bTime  ;<br>       log.info(“cost time” + “—–&gt;”  +time+ “ms”);<br>    }</p>
<pre><code>public static String util<span class="params">(Long startTime,int time2)</span> {
    Date date = new Date<span class="params">(startTime)</span>;
    date.setMinutes<span class="params">(date.getMinutes<span class="params">()</span>+ time2)</span>;
    String  tmp = String.valueOf<span class="params">(date.getTime<span class="params">()</span>)</span>;
    return tmp;
}
<span class="comment">//3.根据时间,ip范围,协议类型,url域名查询所有符合的记录条数</span>
public static void countByLimits<span class="params">(Long stime,int b,int s,String netType,String sip,String eip,String url)</span> throws IOException{
    Long bTime = System.currentTimeMillis<span class="params">()</span>;
    stime = startTime;
    String rstime = util<span class="params">(stime, b)</span>;
    String retime =util<span class="params">(stime, s)</span>;
    FilterList filterList = new FilterList<span class="params">(FilterList.Operator.MUST_PASS_ALL)</span>;
    filterList.addFilter<span class="params">(new SingleColumnValueFilter<span class="params">(<span class="string">"ip"</span>.getBytes<span class="params">()</span>,<span class="string">"ip"</span>.getBytes<span class="params">()</span> , CompareFilter.CompareOp.GREATER, new BinaryComparator<span class="params">(sip.getBytes<span class="params">()</span>)</span>)</span>)</span>;
    <span class="comment">//+Convert.bytesToIpByReg(Convert.ipToBytesByReg(sip))</span>
    filterList.addFilter<span class="params">(new SingleColumnValueFilter<span class="params">(<span class="string">"ip"</span>.getBytes<span class="params">()</span>,<span class="string">"ip"</span>.getBytes<span class="params">()</span> , CompareFilter.CompareOp.LESS, new BinaryComparator<span class="params">(eip.getBytes<span class="params">()</span>)</span>)</span>)</span>;
    filterList.addFilter<span class="params">(new SingleColumnValueFilter<span class="params">(<span class="string">"data"</span>.getBytes<span class="params">()</span>,<span class="string">"net_type"</span>.getBytes<span class="params">()</span> , CompareFilter.CompareOp.EQUAL, new BinaryComparator<span class="params">(netType.getBytes<span class="params">()</span>)</span>)</span>)</span>;
    filterList.addFilter<span class="params">(new SingleColumnValueFilter<span class="params">(<span class="string">"data"</span>.getBytes<span class="params">()</span>,<span class="string">"website"</span>.getBytes<span class="params">()</span> , CompareFilter.CompareOp.EQUAL, new BinaryComparator<span class="params">(url.getBytes<span class="params">()</span>)</span>)</span>)</span>;;
    Scan scan = new Scan<span class="params">(rstime.getBytes<span class="params">()</span> ,retime.getBytes<span class="params">()</span>)</span>;
    scan.setFilter<span class="params">(filterList)</span>;
      ResultScanner rs = table.getScanner<span class="params">(scan)</span>;
        int num = <span class="number">0</span>;
       <span class="keyword">for</span><span class="params">(Result r:rs)</span>{
           num++;
           <span class="keyword">for</span>  <span class="params">(Cell cell : r.rawCells<span class="params">()</span>)</span> {
               System. out .println<span class="params">(
                        <span class="string">"Rowkey : "</span> +Bytes.toString<span class="params">(r.getRow<span class="params">()</span>)</span>.split<span class="params">(<span class="string">"-"</span>)</span>[<span class="number">0</span>]+<span class="string">"-"</span>+ Convert.bytesToIpByReg<span class="params">(Bytes.copy<span class="params">(cell.getRowArray<span class="params">()</span>,cell.getRowOffset<span class="params">()</span>+cell.getRowLength<span class="params">()</span>-<span class="number">4</span>,<span class="number">8</span>)</span>)</span>+
                         <span class="string">" family:"</span> +Bytes.toString<span class="params">(cell.getFamily<span class="params">()</span>)</span>+
                         <span class="string">"   Familiy:Quilifier : "</span> +Bytes. toString <span class="params">(CellUtil. cloneQualifier <span class="params">(cell)</span>)</span>+
                         <span class="string">"   Value : "</span> +Bytes. toString <span class="params">(CellUtil. cloneValue <span class="params">(cell)</span>)</span>+
                        <span class="string">"Time : "</span> +cell.getTimestamp<span class="params">()</span>
                        )</span>;
            }
       }        
    System.out.println<span class="params">(<span class="string">"数据生成16-18分钟,协议为HTTP,URL为google.com,ip范围为40.1.1.1-50.1.1.225的记录:"</span>+num)</span>;
     Long sTime = System.currentTimeMillis<span class="params">()</span>;
       Long time = sTime- bTime ;
       <span class="built_in">log</span>.info<span class="params">(<span class="string">"cost time"</span> + <span class="string">"-----&gt;"</span>  +time+ <span class="string">"ms"</span>)</span>;
}
</code></pre><p>####2.2.3程序效果图<br><img src="/IDC/02.png" alt=""></p>
<p><img src="/IDC/03.png" alt=""></p>
<p><img src="/IDC/04.png" alt=""></p>
<p><img src="/IDC/05.png" alt=""></p>
<p><img src="/IDC/06.png" alt=""></p>
<p>##3.详细设计</p>
<p>###3.1 数据库分析与设计<br><img src="/IDC/07.png" alt=""></p>
<p>###3.2 Rowkey设计<br>rowkey的设计会根据项目的需求进行详细设计，一般高位用于预分区，防止数据写入热点，节点中负载不均衡，由于此次实验在单节点环境下进行，所以并没有下去预分区，而是将timestamp设计在高位，由于对时间范围作范围查询要求，这样设计可以大大提高查询效率， rowkey中ip正好可以使其rowkey保持唯一性，因为会对ip进行范围查询，因此还是单独的放在了column family中。除此之外需要注意的是，rowkey要遵循唯一性，rowkey是一个二进制码流，在此项目中，我们需要把ip转化二进制的byte类型。保证rowkey的长度唯一。</p>
<p>##4.分析和总结</p>
<p>###4.1 系统存在的问题及解决方案<br>1.负载均衡<br>Rowkey是按时间戳的方式递增，一般不要将时间戳放在二进制码的前面，建议将Rowkey的高位作为散列字段，由程序循环生成，低位放时间字段，这样将提高数据均衡分布在每个Regionserver实现负载均衡的几率。如果没有散列字段，首字段直接是时间信息将产生所有新数据都在一个 RegionServer上堆积的热点现象，这样在做数据检索的时候负载将会集中在个别RegionServer，降低查询效率。由于，这次此次实验室在单节点环境中，所以没有也没法考虑热点写入，也没有进行分区，但有一点根据查询要求能很快的获取乐观的结果花费很少的时间。<br>2.程序查询<br>在查询方面暂且用的Scan和Filte，查询效率一般，除此之外还可以用mapreduce进行数据查询以及用更好的方法coprocessor二级索引，由于能力有限，暂且用比较简单的方法，虽然和coprocessor二级索引不能相提并论，但在小数据集中已经够用。在接下来的学习中会花时间去了解并学习如何使用coprocessor完成条件查询。</p>

      
    </div>
    <footer>
      
        <div class="alignleft">
            <a href="/2016/03/30/IDC/#more" class="more-link"><i class="fa fa-chevron-right"></i>Read More</a>
        </div>
        
        
          <div class="alignright"> 
            <a href="http://blucelyen.github.io/2016/03/30/IDC/#comment" class="comment-link">
              <i class="fa fa-comments"></i><span class="ds-thread-count" data-thread-key="/2016/03/30/IDC/">&nbsp;
              </span>
            </a>
          </div>
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
    <div class="gallery">
  <div class="photoset">
    
      <img src="">
    
  </div>
  <div class="control">
    <div class="prev"></div>
    <div class="next"></div>
  </div>
</div>
  
  <div class="post-content">
    <header>
      
      <time datetime="2016-03-30T06:59:44.360Z"><a href="/2016/03/30/Coprocessor/">Mié., Mar. 30 2016, 2:59:44 pm</a></time>

  
    <h1 class="title"><a href="/2016/03/30/Coprocessor/">Hbase Coprocessor</a></h1>
  



<div class="clear"></div>
      
    </header>
    <div class="entry">
      
        <p>##1.why Hbase Coprocessor?<br>  HBase作为列族数据库最经常被人诟病的特性包括：无法轻易建立“二级索引”，难以执行求和、计数、排序等操作。比如，在旧版(&lt;0.92)Hbase中，统计数据表的总行数，需要使用Counter方法，执行一次MapReduce Job才能得到。虽然HBase在数据存储层中集成了MapReduce，能够有效用于数据表的分布式计算。然而在很多情况下，做一些简单的相加或者聚合计算的时候，如果直接将计算过程放置在server端，能够减少通讯开销，从而获得很好的性能提升。于是，HBase在0.92之后引入了协处理器(coprocessors)，实现一些激动人心的新特性：能够轻易建立二次索引、复杂过滤器(谓词下推)以及访问控制等。<a href="http://blog.csdn.net/hljlzc2007/article/details/12652243" target="_blank" rel="external">Coprocessor详细介绍</a>.  </p>
<p>##2.应用场景</p>
<p>###2.1基于IDC用户网站访问记录系统设计与开发<br>1.用Coprocessor实现查询符合某一时间段内访问网站的记录总数.</p>
<p>Tips:</p>
<p>HBase的coprocessor分为两类，Observer和EndPoint。其中Observer相当于触发器，EndPoint相当于存储过程。其中Observer的代码部署在服务端，相当于对API调用的代理。介绍这方面的文章不少,例如<a href="http://blog.csdn.net/hljlzc2007/article/details/12652243" target="_blank" rel="external">Coprocessor详细介绍</a>.  但是关于EndPoint的使用。0.94.x之前使用EndPoint需要实现CoprocessorProtocol接口，而0.96.x的EndPoint改为用protobufs作为RPC的协议。<a href="http://blog.csdn.net/caisini_vc/article/details/5599468" target="_blank" rel="external">ptotobuf协议详解</a>,下面将会讲解如何使用新版的Endpoint.</p>
<p>###2.2实现步骤</p>
<p>####2.2.1protobuf安装与配置<br>1.在Ubuntu的终端里输入:wget <a href="http://protobuf.googlecode.com/filesprotobuf-2.5.0.tar.gz" target="_blank" rel="external">http://protobuf.googlecode.com/filesprotobuf-2.5.0.tar.gz</a></p>
<p>2.下载之后解压:tar -xvzf protobuf-2.5.0.tar.gz </p>
<p>3.进入到解压后的目录:cd protobuf-2.5.0</p>
<p>4.执行命令: ./configure –prefix=/usr/local/protobuf</p>
<p>5.执行命令:make</p>
<p>6.执行命令:make check</p>
<p>7.执行命令:make install(权限不够记得sudo make install)</p>
<p>8.修改配置文件:sudo vim ~/.profile并再文件末尾添加</p>
<p>export PROTOBUF_HOME=$PROTOBUF_HOME:/usr/local/bin export PATH=$PATH:/usr/local/protobuf/bin/</p>
<p>export PKG_CONFIG_PATH=/usr/local/protobuf/lib/pkgconfig/</p>
<p>9.更新~/.profile文件:source ~/.profile</p>
<p>10.检测是否安装成功:protoc –version 出现libptotoc 2.5.0则安装成功.</p>
<p>####2.2.2根据项目需求查询条件编写protobuf文件并编译<br>1.在此应用场景中用Coprocessor实现查询符合某一时间段内访问网站的记录总数所对应的protobuf文件如下:</p>
<pre><code><span class="keyword">option</span> java_package = <span class="string">"com.lyen.coprocessor02"</span>;
<span class="keyword">option</span> java_outer_classname = <span class="string">"ServerCopro02"</span>;
<span class="keyword">option</span> java_generic_services = <span class="literal">true</span>;
<span class="keyword">option</span> java_generate_equals_and_hash = <span class="literal">true</span>;
<span class="keyword">option</span> optimize_for = SPEED;

<span class="class"><span class="keyword">message</span> <span class="title">RowCountRequest</span> </span>{
<span class="keyword">required</span> <span class="built_in">int64</span> starttime = <span class="number">1</span>;
<span class="keyword">required</span> <span class="built_in">int64</span> endtime = <span class="number">2</span>;
}

<span class="class"><span class="keyword">message</span> <span class="title">RowCountResponse</span> </span>{
<span class="keyword">required</span> <span class="built_in">int64</span> rownum = <span class="number">1</span>;
}

<span class="class"><span class="keyword">service</span> <span class="title">CopCountService</span> </span>{
<span class="function"><span class="keyword">rpc</span> getCount(RowCountRequest)
<span class="keyword">returns</span> (RowCountResponse)</span>;
</code></pre><p>   }</p>
<p>2.编译:protoc –java_out=/home/lyen proto2.pro编译后会生成一个包里边包含ServerCopro02.java,将此包复制到你的eclipse工程下即可使用.</p>
<p>3.RowCountRequest是发送给服务端的消息，这里定义int类型starttime和endtime来存放具体请求消息内容(查询条件)。RowCountResponse是返回的结果，统计的是行数，所以用int类型存放。CopCountService中定义一个方法getCount，传递请求，返回响应.</p>
<p>Tips:<br>a.不同的查询条件需要编写不同的protobuf文件,这是比较麻烦的,不过有了这个例子其余的都是照着葫芦画瓢.</p>
<p>####2.2.3实现Edpoint服务端<br>    public class MyEndpoint02 extends ServerCopro02.CopCountService implements Coprocessor, CoprocessorService{</p>
<pre><code><span class="keyword">private</span> RegionCoprocessorEnvironment environment = <span class="keyword">null</span>;
@Override
<span class="keyword">public</span> Service getService() {
    <span class="keyword">return</span> <span class="keyword">this</span>;
}

@Override
<span class="keyword">public</span> <span class="keyword">void</span> start(CoprocessorEnvironment env) <span class="keyword">throws</span> IOException {
    <span class="keyword">if</span> (env <span class="keyword">instanceof</span> CoprocessorEnvironment) {
        <span class="keyword">this</span>.environment = (RegionCoprocessorEnvironment) env;
    } <span class="keyword">else</span> {
        <span class="keyword">throw</span> <span class="keyword">new</span> CoprocessorException(<span class="string">"Must be loaded on a table region!"</span>);
    }
}

@Override
<span class="keyword">public</span> <span class="keyword">void</span> stop(CoprocessorEnvironment arg0) <span class="keyword">throws</span> IOException {

}

@Override
<span class="keyword">public</span> <span class="keyword">void</span> getCount(RpcController controller, RowCountRequest request, RpcCallback&lt;RowCountResponse&gt; done) {
    RegionScanner scanner = <span class="keyword">null</span>;
    RowCountResponse.Builder respBuilder = RowCountResponse.newBuilder();
    <span class="keyword">Long</span> stime = request.getStarttime();
    <span class="keyword">Long</span> etime = request.getEndtime();
    <span class="keyword">long</span> <span class="keyword">count</span> = <span class="number">0</span>;
    <span class="keyword">try</span> {
        <span class="comment">/*
         * 此次coprocessor作业中遇到的一个比较坑爹的问题如下
         * 我再把数据写入库的时候,时间戳和ip作为rowkey,然而我是把时间戳这个字符串(1451023295513).getBytes()
         * 其实应该直接Bytes.toBytes(1451023295513),这两种方法得到的结果是不一样的再控制台输出亲测不一致,我
         * 在写入库的时候用的第一种方法,而在查询是用的第二种方法,以致于浪费很多时间找错,体会到了规范编程的重
         * 要.
         * 
         */</span>

        String s = String.valueOf(stime);
        String e = String.valueOf(etime);
        Scan scan = <span class="keyword">new</span> Scan(s.getBytes(),e.getBytes());
        scan.setMaxVersions(<span class="number">1</span>);
        scan.setFilter(<span class="keyword">new</span> FirstKeyOnlyFilter());
        HRegion rg = (HRegion) environment.getRegion();
        scanner = rg.getScanner(scan);
        List&lt;Cell&gt; list = <span class="keyword">new</span> ArrayList&lt;Cell&gt;();
        <span class="keyword">while</span> (scanner.<span class="keyword">next</span>(list)){
            <span class="comment">//System.out.println(list.size());</span>
            <span class="keyword">count</span> += <span class="number">1</span>; 
        }
        <span class="keyword">if</span>(list.<span class="keyword">size</span>() != <span class="number">0</span>){
            <span class="keyword">count</span> = <span class="keyword">count</span> +<span class="number">1</span>;
        }
    } <span class="keyword">catch</span> (IOException e) {
        e.printStackTrace();
    } <span class="keyword">finally</span> {
        <span class="keyword">if</span> (scanner != <span class="keyword">null</span>) {
            <span class="keyword">try</span> {
                scanner.close();
            } <span class="keyword">catch</span> (IOException e) {
                e.printStackTrace();
            }
        }
    }
    respBuilder.setRownum(<span class="keyword">count</span>);
    done.run(respBuilder.build());
}

}
</code></pre><p>Tips:</p>
<p>a.MyEndpoint02需要继承抽象类ServerCopro02.CopCountService并实现Coprocessor和CoprocessorService接口。CopCountService在刚才生成的包里。</p>
<p>b.start和stop方法分别负责endpoint执行前的初始化和结束后的清理工作。start方法的参数是一个接口，需要根据实际环境将其转成需要的类型。</p>
<p>c.主要需要实现的是getCount方法，这也刚才在protobuf中定义的方法。服务端收到请求信息starttime和endtime，那么返回这段时间内符合条件的查询结果；统计region的记录行数并返回。</p>
<p>####2.2.3实现客户端<br>    public class MyClient02 {<br>    public static  Configuration conf;<br>    private byte[] table_name = “exp”.getBytes();<br>    public  Long starttime=new Long(1451023295513L + 2<em>60</em>1000);<br>    public  Long endtime=new Long(1451023295513L + 3<em>60</em>1000) ;<br>    //设置好请求信息(查询条件starttime和endtime)<br>    final ServerCopro02.RowCountRequest request =   ServerCopro02.RowCountRequest.newBuilder().setStarttime(starttime).setEndtime(endtime).build();<br>    static {<br>        conf = HBaseConfiguration.create();<br>        conf.set(“hbase.zookeeper.quorum”, “localhost”);<br>        conf.set(“hbase.zookeeper.property.clientPort”, “2181”);<br>    }<br>    public void my() throws ServiceException, Throwable{<br>        long startTime = System.currentTimeMillis();<br>        HTable table = new     HTable(conf,table_name);<br>        //方法的返回值是Map类型，Map的size与参与计算的region个数一致。所以最后需要做的一步是讲返回结果进行累加，得到最后的结果。<br>        Map<byte[], long=""> result = table.coprocessorService(ServerCopro02.CopCountService.class,null,null,new Batch.Call<servercopro02.copcountservice,long>() {<br>            @Override<br>            public Long call(CopCountService counter) throws IOException {<br>                ServerRpcController controller = new ServerRpcController();<br>                BlockingRpcCallback<servercopro02.rowcountresponse> rpccall = new  BlockingRpcCallback<servercopro02.rowcountresponse>();<br>                counter.getCount(controller, request, rpccall);<br>                ServerCopro02.RowCountResponse resp = rpccall.get();<br>                return resp.getRownum();<br>            }<br>        }<br>        );<br>         long line = 0;<br>            for (long l : result.values()){<br>                line += l;<br>            }<br>            long endTime = System.currentTimeMillis();<br>            long time = endTime-startTime;</servercopro02.rowcountresponse></servercopro02.rowcountresponse></servercopro02.copcountservice,long></byte[],></p>
<pre><code>        System.out.<span class="built_in">println</span>(<span class="string">"符合2-3分钟的数据所消耗的时间以及条数"</span>);
        System.out.<span class="built_in">println</span>(<span class="string">"cost:"</span>+time+<span class="string">"ms"</span> );
        System.out.<span class="built_in">println</span>(<span class="string">"lines: "</span> + <span class="built_in">line</span>);
}
<span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> main(<span class="keyword">String</span>[] args) <span class="keyword">throws</span> ServiceException, Throwable {
    MyClient02 mc = <span class="keyword">new</span> MyClient02();
    mc.my();
}
}
</code></pre><p>Tips:</p>
<pre><code><span class="keyword">public</span> &lt;T <span class="keyword">extends</span> Service, R&gt; Map&lt;<span class="keyword">byte</span>[],R&gt; coprocessorService(<span class="keyword">final</span> <span class="keyword">Class</span>&lt;T&gt; service,
<span class="keyword">byte</span>[] startKey, <span class="keyword">byte</span>[] endKey, <span class="keyword">final</span> Batch.<span class="keyword">Call</span>&lt;T,R&gt; callable)
</code></pre><p>该方法有四个参数，第1个参数是protobuf生成的ServerCopro02.CopCountService.class类对象。第2个和第3个参数分别为起始和结束rowkey，这里的意思是范围内rowkey所在的region都会调用endpoint，这里设为null表明所有的region都会调用。第4个参数为接口，需要重写call方法。</p>
<p>####2.2.4部署<br>1.首先把自己写的服务端MyClient02类和之前用protobuf生成的CopCountService协议类一起打包成一个jar包放在Hbase安装目录的lib目录下,然后重新启动Hbase.并且也要把这个jar包放一份在自己的java工程中.</p>
<p>2.启动hbase shell通过shell方式(作用于指定)</p>
<p>import com.lyen.coprocessor02.MyEndpoint02</p>
<p>alter ‘exp’,’coprocessor’=&gt;’|com.lyen.coprocessor02.MyEndpoint02|1001|’</p>
<p>####2.2.5运行客户端程序<br><img src="/coprocessor/0001.png" alt=""></p>
<p>##3.总结</p>
<p>###3.1不足之处<br>a.考虑自身笔记本配置的限制,在之前IDC用户网站访问记录系统设计与开发的时候在一台机器上搭建的Hadoop伪分布环境,在Hbase的时候并没有进行数据的散列即预分区,假如说采用预分区散列数据的话,程序的改动也仅仅是在服务端的Endpoint,若采用与分区则我们在写Endpoint时候是需要修改如下代码</p>
<pre><code><span class="label">HRegion</span> rg = (HRegion) environment.getRegion()<span class="comment">;</span>
<span class="label">rg.getStartKey</span>(<span class="keyword">byte[] </span><span class="keyword">b); </span>
<span class="label">rg.getEndKey</span>(<span class="keyword">byte[] </span><span class="keyword">b); </span>
</code></pre><p>//这两个方法是为了获取分区后的散列号(而且此方法只有habase0.98才有),以免不必要的麻烦推荐Hbase使用0.98版本的.</p>
<p>然后再重组RowKey,(散列号+timestamp+ip)和最初表结构一致的RowKey.</p>
<p>###3.1难点<br>a.Coprocessor的优点想必在此博客的开始就已经了解过了吧,真正难的地方在于很难调试,毕竟Endpoint放在服务器端运行,一出错就得又改代码并重新部署,而且出错在哪儿只有看日志文件慢慢找,客户端是无法看到的,确实很蛋疼,不过慢慢的还是会习惯的,此博客写的是比较简单的服务器端的求和的小程序,因此调试起来也比较容易,外加了两个开始时间和技术时间这两个参数是为了更容易理解新版Endpoint的写法.关于Coprocessor的更多场景应用还有待深入学习.</p>

      
    </div>
    <footer>
      
        <div class="alignleft">
            <a href="/2016/03/30/Coprocessor/#more" class="more-link"><i class="fa fa-chevron-right"></i>Read More</a>
        </div>
        
        
          <div class="alignright"> 
            <a href="http://blucelyen.github.io/2016/03/30/Coprocessor/#comment" class="comment-link">
              <i class="fa fa-comments"></i><span class="ds-thread-count" data-thread-key="/2016/03/30/Coprocessor/">&nbsp;
              </span>
            </a>
          </div>
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
    <div class="gallery">
  <div class="photoset">
    
      <img src="">
    
  </div>
  <div class="control">
    <div class="prev"></div>
    <div class="next"></div>
  </div>
</div>
  
  <div class="post-content">
    <header>
      
      <time datetime="2016-03-30T02:05:38.757Z"><a href="/2016/03/30/TriangleCount/">Mié., Mar. 30 2016, 10:05:38 am</a></time>

  
    <h1 class="title"><a href="/2016/03/30/TriangleCount/">TriangleCount(三角计数)</a></h1>
  



<div class="clear"></div>
      
    </header>
    <div class="entry">
      
        <p>计算出关系图中，相互联系的三角结构（三顶点中任意两个顶点都可到达）个数。其个数衡量可用作个人或社区群体的稳定程度。</p>
<h3 id="1-_有向图转化为无向图">1. 有向图转化为无向图</h3><p>如下图所示，社交网络中的关注关系一般为有向图，这里需要转化为无向图。转化思路为：如果IF (A-&gt;B) or (B-&gt;A) THEN A-B。只要A与B在有向图中存在一条有向边，便认为A与B在无向图中存在边。（有向图弱化为无向图，即顶点间有关系即可）</p>
<p><img src="/triangleCount/01.png" alt=""><br>
      
    </div>
    <footer>
      
        <div class="alignleft">
            <a href="/2016/03/30/TriangleCount/#more" class="more-link"><i class="fa fa-chevron-right"></i>Read More</a>
        </div>
        
        
          <div class="alignright"> 
            <a href="http://blucelyen.github.io/2016/03/30/TriangleCount/#comment" class="comment-link">
              <i class="fa fa-comments"></i><span class="ds-thread-count" data-thread-key="/2016/03/30/TriangleCount/">&nbsp;
              </span>
            </a>
          </div>
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
    <div class="gallery">
  <div class="photoset">
    
      <img src="">
    
  </div>
  <div class="control">
    <div class="prev"></div>
    <div class="next"></div>
  </div>
</div>
  
  <div class="post-content">
    <header>
      
      <time datetime="2015-09-29T07:10:01.185Z"><a href="/2015/09/29/k-means/">Mar., Sep. 29 2015, 3:10:01 pm</a></time>

  
    <h1 class="title"><a href="/2015/09/29/k-means/">K-Means算法</a></h1>
  



<div class="clear"></div>
      
    </header>
    <div class="entry">
      
        <p>首先要来了解的一个概念就是聚类，简单地说就是把相似的东西分到一组，同 Classification (分类)不同， 对于一个 classifier ，通常需要你告诉它“这个东西被分为某某类”这样一些例子，理想情况下，一个 classifier 会从它得到的训练集中进行“学习”，从而具备对未知数据进行分类的能力，这种提供训练数据的过程通常叫做 supervised learning (监督学习)，而在聚类的时候，我们并不关心某一类是什么，我们需要实现的目标只是把相似的东西聚到一起，因此，一个聚类算法通常只需要知道如何<a href="http://wenku.baidu.com/link?url=D6Anniohd8zD0KF7HfUZEDeUxtXj6RwWt5UDYmDdvLIibuOWDhOSW1DVVugOAT9HOnr3rKL3rZ-UMLPexQngSQ9Jyv83X_yR33HgC-HQM-i">计算相似度</a>就可以开始工作了。ps:（不同的相似度计算方法应用于不同的场景，在聚类中应充分考虑使用哪一种相似度计算方法）。因此 clustering 通常并不需要使用训练数据进行学习，这在 Machine Learning 中被称作 unsupervised learning (无监督学习)。</p>
<h3 id="1-算法基本简介">1.算法基本简介</h3><p>k-means 算法接受输入量 k ；然后将n个数据对象划分为 k个聚类以便使得所获得的聚类满足：同一聚类中的对象相似度较高；而不同聚类中的对象相似度较小。聚类相似度是利用各聚类中对象的均值所获得一个“中心对象”（引力中心）来进行计算的。</p>
      
    </div>
    <footer>
      
        <div class="alignleft">
            <a href="/2015/09/29/k-means/#more" class="more-link"><i class="fa fa-chevron-right"></i>Read More</a>
        </div>
        
        
          <div class="alignright"> 
            <a href="http://blucelyen.github.io/2015/09/29/k-means/#comment" class="comment-link">
              <i class="fa fa-comments"></i><span class="ds-thread-count" data-thread-key="/2015/09/29/k-means/">&nbsp;
              </span>
            </a>
          </div>
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
    <div class="gallery">
  <div class="photoset">
    
      <img src="">
    
  </div>
  <div class="control">
    <div class="prev"></div>
    <div class="next"></div>
  </div>
</div>
  
  <div class="post-content">
    <header>
      
      <time datetime="2015-09-29T07:10:01.177Z"><a href="/2015/09/29/ALS/">Mar., Sep. 29 2015, 3:10:01 pm</a></time>

  
    <h1 class="title"><a href="/2015/09/29/ALS/">ALS协同过滤算法</a></h1>
  



<div class="clear"></div>
      
    </header>
    <div class="entry">
      
        <h3 id="1-简介:">1.简介:</h3><p> 协同过滤常被应用于推荐系统，旨在补充用户-商品关联矩阵中所缺失的部分。MLlib当前支持基于模型的协同过滤，其中用户和商品通过一小组隐语义因子进行表达，并且这些因子也用于预测缺失的元素。Spark MLlib实现了交替最小二乘法 (ALS) 来学习这些隐性语义因子。在 MLlib 中的实现有如下的参数:</p>
<pre><code><span class="number">1</span><span class="class">.numBlocks</span> 是用于并行化计算的分块个数 (设置为-<span class="number">1</span>，为自动配置)。
<span class="number">2</span><span class="class">.rank</span> 是模型中隐语义因子的个数。
   ps: 用户根据对物品的一些属性对物品的评价情况，比如用户买一个商品是因为它美观，实用        
   那么美观，实用这两个因素就是模型中的隐语因子，即<span class="number">2</span>个隐语因子隐语义模型其实就是用户
   评价数据的因子分析，得到各个商品的主要因子，用的原理都是<span class="function"><span class="title">SVD</span><span class="params">(奇异值分解)</span></span>
<span class="number">3</span><span class="class">.iterations</span> 是迭代的次数。
<span class="number">4</span><span class="class">.lambda</span> 是ALS的正则化参数。
<span class="number">5</span>. implicitPrefs 决定了是用显性反馈ALS的版本还是用适用隐性反馈数据集的版本。
<span class="number">6</span><span class="class">.alpha</span> 是一个针对于隐性反馈 ALS 版本的参数，这个参数决定了偏好行为强度的基准。
</code></pre>
      
    </div>
    <footer>
      
        <div class="alignleft">
            <a href="/2015/09/29/ALS/#more" class="more-link"><i class="fa fa-chevron-right"></i>Read More</a>
        </div>
        
        
          <div class="alignright"> 
            <a href="http://blucelyen.github.io/2015/09/29/ALS/#comment" class="comment-link">
              <i class="fa fa-comments"></i><span class="ds-thread-count" data-thread-key="/2015/09/29/ALS/">&nbsp;
              </span>
            </a>
          </div>
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>





<nav id="pagination">
  
  
  <div class="clearfix"></div>
</nav>

<script src="/js/jquery.anystretch.min.js" type="text/javascript"></script>
<script src="/js/cover.js" type="text/javascript"></script>

    <div class="clearfix"></div>
  </div>
  <footer id="footer" class="inner"><div>
  
  &copy; 2016 Lyen
  
</div>
Powered by <a href="http://zespia.tw/hexo/" title="Hexo" target="_blank" rel="external">Hexo</a> and <a href="http://pages.github.com/" title="GitHub Pages" target="_blank" rel="external">GitHub Pages</a>

<div class="clearfix"></div></footer>
  
<script src="/js/jquery.imagesloaded.min.js" type="text/javascript"></script>
<script src="/js/gallery.js" type="text/javascript"></script>
<script src="//netdna.bootstrapcdn.com/bootstrap/3.1.0/js/bootstrap.min.js" type="text/javascript"></script>




    <script type="text/javascript">
        (function(){

            $(window).scroll(function(){

                var scrollTop = $(window).scrollTop();
                if ( scrollTop >200 ){
                    $("#main-nav").removeClass('normal_mode').addClass('top_mode');
                } else{
                    $("#main-nav").removeClass('top_mode').addClass('normal_mode');
                }

            });

        })();
    </script>



  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css" media="screen" type="text/css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>
  <script type="text/javascript">
  (function($){
    $('.fancybox').fancybox({
      'titlePosition': 'inside'
    });
  })(jQuery);
  </script>



    
    <script type="text/javascript">
      var duoshuoQuery = {short_name:"BluceLyen"};
    (function() {
      var ds = document.createElement('script');
      ds.type = 'text/javascript';ds.async = true;
      ds.src = 'http://static.duoshuo.com/embed.js';
      ds.charset = 'UTF-8';
      (document.getElementsByTagName('head')[0] 
      || document.getElementsByTagName('body')[0]).appendChild(ds);
    })();
  </script>



<script type="text/javascript">
  
  $(function(){

    $('.title').hover(
      function() {      
        $(this).stop().animate(
          {'marginLeft': '10px'}, 200
        );   
      }, 
      function() {       
        $(this).stop().animate({'marginLeft': '0px'}, 200);      
      
    });   

  });

</script>


</body>
</html>