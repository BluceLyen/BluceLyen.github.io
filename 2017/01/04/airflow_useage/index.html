<!DOCTYPE HTML>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Lyen Cc</title>
  <meta name="author" content="Lyen">
  
  <meta name="description" content="Airflow usage1. airflow1.1 简介Airflow是一个工作流调度器，相对于传统的crontab任务管理，Airflow很好的为我们理清了复杂的任务依赖关系、监控任务执行的情况Airflow提供一个非常容易定义DAG的机制：一个开发者使用Python 脚本定义他的DAG。然后自">
  
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  
  <meta property="og:site_name" content="Lyen Cc"/>

  
    <meta property="og:image" content="undefined"/>
  

  <link href="/favicon.ico" rel="icon">
  <link rel="alternate" href="/atom.xml" title="Lyen Cc" type="application/atom+xml">
  <link rel="stylesheet" href="//netdna.bootstrapcdn.com/bootstrap/3.1.0/css/bootstrap.min.css" type="text/css">
<link rel="stylesheet" href="//netdna.bootstrapcdn.com/font-awesome/4.0.3/css/font-awesome.css" type="text/css">
<link rel="stylesheet" href="/css/style.css" type="text/css">
  <script src="http://code.jquery.com/jquery-2.1.1.min.js"></script>
  <!--[if lt IE 9]><script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]-->
  
<script type="text/javascript">
  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-368771XX-X']);
  _gaq.push(['_trackPageview']);

  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();
</script>

</head>

<body>
  <header id="header" class='normal_mode'>
    <nav id="main-nav">
  <ul class='container'>
    
      <li><a href="/">Home</a></li>
    
      <li><a href="/archives">Archives</a></li>
    
  </ul>
  <div class="clearfix"></div>
</nav>
  </header>
  <div id="content" class="container">
    <article class="post">
  
    <div class="gallery">
  <div class="photoset">
    
      <img src="">
    
  </div>
  <div class="control">
    <div class="prev"></div>
    <div class="next"></div>
  </div>
</div>
  
  <div class="post-content">
    <header>
      
      <time datetime="2017-01-04T01:22:22.219Z"><a href="/2017/01/04/airflow_useage/">Mié., Ene. 4 2017, 9:22:22 am</a></time>

  
    <h1 class="title"></h1>
  



<div class="clear"></div>
      
    </header>
    <div class="entry">
      
        <h2 id="Airflow_usage">Airflow usage</h2><h3 id="1-_airflow">1. airflow</h3><h4 id="1-1_简介">1.1 简介</h4><p>Airflow是一个工作流调度器，相对于传统的crontab任务管理，Airflow很好的为我们理清了复杂的任务依赖关系、监控任务执行的情况Airflow提供一个非常容易定义DAG的机制：一个开发者使用Python 脚本定义他的DAG。然后自动加载这个DAG到DAG引擎，为他的首次运行进行调度。修改一个DAG就像修改Python 脚本一样容易，airflow 是能进行数据pipeline的管理，甚至是可以当做更高级的cron job 来使用,airflow是用python写的，它能进行工作流的调度，提供更可靠的流程，并且自带UI</p>
<h4 id="1-2_概念">1.2 概念</h4><hr>
<h2 id="Operators:">Operators:</h2><p>基本可以理解为一个抽象化的task， Operator加上必要的运行时上下文就是一个task， 有三类</p>
<ol>
<li><p>Sensor(传感监控器)，监控一个事件的发生</p>
</li>
<li><p>Trigger(或者叫做Remote Excution)，执行某个远端动作， (我在代码中没有找到这个类别)</p>
</li>
<li><p>Data transfer(数据转换器)，完成数据转换</p>
</li>
</ol>
<h2 id="Hooks:">Hooks:</h2><p>Hook是airflow与外部平台/数据库交互的方式，一个Hook类就像是一个JDBC driver一样，airflow已经实现了jdbc/ftp/http/webhdfs/ssh很多hook</p>
<p>Tasks: task代表DAG中的一个节点，它其实是一个BaseOperator子类</p>
<p>Task instances，即task的运行态实例，它包含了task的status(成功/失败/重试中/已启动)</p>
<p>Job: Airflow中Job很少提及，但在数据库中有个job表， 需要说明的是Job和task并不是一回事，Job可以简单理解为Airflow的批次， 更准确的说法是同一批被调用task或dag的统一代号，有三类Job， 分别SchedulerJob/BackfillJob/LocalTaskJob，对于SchedulerJob和BackfillJob， job指的是指定dag这次被调用的运行时代号，LocalTaskJob是指定task的运行时代号</p>
<h2 id="Connections:">Connections:</h2><p>我们的Task需要通过Hook访问其他资源，Hook仅仅是一种访问方式，就像是JDBC driver一样，要连接DB，我们还需要DB的IP/Port/User/Pwd等信息， 这些信息不太适合hard code在每个task中，可以把它们定义成Connection， airflow将这些connection信息存放在后台的connection表中。我们可以在WebUI的Admin-&gt;Connections管理这些连接</p>
<p>详细概念与操作参见 <a href="http://www.codesec.net/view/222868.html" target="_blank" rel="external">系统研究Airbnb开源项目airflow</a></p>
<h3 id="2-_安装">2. 安装</h3><p>airflow 需要一个主目录，默认为~ /airflow，也可以安装在任何你想安装的位置，建议在自己的主目录下主动创建一个airflow文件夹用于安装，安装成功后airflow.cfg配置文件，目录dags，以及日志logs都位于~ /airflow</p>
<p>PS:dags目录没有则自行创建，所有的airflow python脚本都放置在此目录。文件系统默认的配置文件是从 airflow包的configuration.py文件中获取的，在设置了路径和airflow.cfg之后，配置将由airflow.cfg所替代</p>
<pre><code><span class="comment">//设置airflow主目录，修改环境变量</span>
<span class="keyword">export</span> AIRFLOW_HOME=~/airflow
<span class="comment">//安装python-mysqldb</span>
sudo apt-get install python-mysqldb 
<span class="comment">//安装airflow</span>
pip install airflow
<span class="comment">//安装airflow组件</span>
pip install airflow[mysql,mysqlclient,hive,password,crypto]
<span class="comment">//初始化数据库</span>
airflow initdb
<span class="comment">//开启web服务器, 默认端口号为8080</span>
airflow webserver -p <span class="number">8080</span> &amp;
</code></pre><p><img src="https://github.com/BluceLyen/BluceLyen.github.io/blob/master/image/webUI.png?raw=true" alt=""></p>
<h3 id="3-_配置mysql数据库">3. 配置mysql数据库</h3><h4 id="3-1_创建mysql_airflow_用户以及数据库">3.1 创建mysql airflow 用户以及数据库</h4><pre><code><span class="operator"><span class="keyword">create</span> <span class="keyword">database</span> airflow;</span>
<span class="operator"><span class="keyword">create</span> <span class="keyword">user</span> <span class="string">'airflow@localhost'</span> <span class="keyword">IDENTIFIED</span> <span class="keyword">BY</span> <span class="string">"123"</span>;</span>
<span class="operator"><span class="keyword">grant</span> all <span class="keyword">privileges</span> <span class="keyword">on</span> airflow.* <span class="keyword">to</span> airflow@localhost <span class="keyword">identified</span> <span class="keyword">by</span> <span class="string">'123'</span>;</span>
<span class="operator"><span class="keyword">flush</span> <span class="keyword">privileges</span>;</span>
</code></pre><h4 id="3-2_编辑airflow-cfg配置文件">3.2 编辑airflow.cfg配置文件</h4><pre><code>vim ~/airflow/airflow<span class="class">.cfg</span>

sql_alchemy_conn=mysql:<span class="comment">//airflow:123@localhost:3306/airflow(修改数据库连接为mysql)</span>
executor = <span class="function"><span class="title">LocalExecutor</span><span class="params">(设置executor为LocalExecutor)</span></span>
airflow <span class="function"><span class="title">initdb</span><span class="params">(初始化数据库)</span></span>
</code></pre><h4 id="3-3_在web_UI中新建本地mysql连接">3.3 在web UI中新建本地mysql连接</h4><p>浏览器输入  localhost:8080  打开Airflow web UI </p>
<p>web UI中找到Admin下的connections<br><img src="https://github.com/BluceLyen/BluceLyen.github.io/blob/master/image/mysqlconn.png?raw=true" alt=""></p>
<p>新建connection，自命名Conn Id，选择连接类型为Mysql，Host为localhost，Schema为airflow，数据库登录名和密码和上边配置的数据库连接一致<br><img src="https://github.com/BluceLyen/BluceLyen.github.io/blob/master/image/mysqlconndetail.png?raw=true" alt=""><br>Data Profiling下Ad Hoc Query简单查询符合条件的DAG</p>
<p><img src="https://github.com/BluceLyen/BluceLyen.github.io/blob/master/image/mysqlquery.png?raw=true" alt=""></p>
<h3 id="4-_配置邮件服务">4. 配置邮件服务</h3><p>邮件服务，方便data pipline中整个DAG调度中运行情况的邮件通知</p>
<p>编辑airflow.cfg配置文件,更改邮件服务器为网易企业邮箱服务器，更改邮件用户名和密码</p>
<pre><code>vim ~/airflow/airflow<span class="class">.cfg</span>

smtp_host = smtp<span class="class">.ym</span>.<span class="number">163</span><span class="class">.com</span>
smtp_starttls = True
smtp_ssl = False
smtp_user = [email username] e<span class="class">.x</span>: lienlian@wolongdata<span class="class">.com</span>
smtp_port = <span class="number">25</span>
smtp_password = [your email password]
smtp_mail_from = [email username]
</code></pre><h3 id="5-第一个简单的airflow">5.第一个简单的airflow</h3><p>使用BashOperator实现一个简单的airflow，更多Operator详见<a href="http://pythonhosted.org/airflow/code.html#operators" target="_blank" rel="external">Airflow官方文档 Operators</a></p>
<h4 id="5-1_编写airflow_python脚本_first_dag-py">5.1 编写airflow python脚本 first_dag.py</h4><p>一个DAG包含多个依赖关系的task，每个DAG和task都有自己独有的id</p>
<pre><code><span class="comment">#encoding:utf-8</span>
<span class="keyword">from</span> airflow <span class="keyword">import</span> DAG
<span class="keyword">from</span> airflow.operators.bash_operator <span class="keyword">import</span> BashOperator
<span class="keyword">from</span> datetime <span class="keyword">import</span> datetime, timedelta

default_args = {
    <span class="string">'owner'</span>: <span class="string">'airflow'</span>,
    <span class="string">'depends_on_past'</span>: <span class="keyword">False</span>,
    <span class="string">'start_date'</span>: datetime(<span class="number">2017</span>, <span class="number">1</span>, <span class="number">2</span>),
    <span class="string">'email'</span>: [<span class="string">'lienlian@wolongdata.com'</span>],
    <span class="string">'email_on_failure'</span>: <span class="keyword">True</span>,
    <span class="string">'email_on_retry'</span>: <span class="keyword">False</span>,
    <span class="string">'retries'</span>: <span class="number">1</span>,
    <span class="string">'retry_delay'</span>: timedelta(minutes=<span class="number">5</span>),
    <span class="comment"># 'queue': 'bash_queue',</span>
    <span class="comment"># 'pool': 'backfill',</span>
    <span class="comment"># 'priority_weight': 10,</span>
    <span class="comment"># 'end_date': datetime(2017, 1,2),</span>
}  


<span class="string">"""
first_bash_dag     -&gt; dag_id
default_args       -&gt; 默认参数
schedule_interval  -&gt; 调度时间
"""</span>
dag = DAG(<span class="string">'first_bash_dag'</span>,default_args=default_args，schedule_interval=<span class="string">'@once'</span>)   
<span class="comment">#打印当前系统时间</span>
bash_cmd_1 = <span class="string">'date'</span>
<span class="comment">#列出当前目录所有文件与目录</span>
bash_cmd_2 = <span class="string">'ls ~/'</span>

<span class="string">"""
同时运行多条bash命令
bash_multi = '(date;ls ~/;top)'
运行bash脚本
 bash_bash = '(bash ~/bash.sh)'
"""</span>
<span class="comment">#DAG第一个task</span>
task_1= BashOperator(task_id=<span class="string">'print_date'</span>,
                         bash_command=bash_cmd_1,
                         dag=dag)
<span class="comment">#DAG第二个task                         </span>
task_2= BashOperator(task_id=<span class="string">'print_ls'</span>,
                         bash_command=bash_cmd_2,
                         dag=dag)
<span class="comment">#设置task之间的依赖关系，task_2在task_1执行成功之后执行</span>
task_1.set_downstream(task_2)
</code></pre><h4 id="5-2_初始化DAG">5.2 初始化DAG</h4><ol>
<li>将编写好的airflow python脚本copy到~/airflow/dags中</li>
<li>python first_dag.py (python执行编写的脚本文件检查是否有语法错误)</li>
<li>airflow initdb 重新初始化数据库</li>
</ol>
<p>初始化数据库后在web UI中你会看见新加载出来的DAG,也可以使用命令airflow list_dags查看当前处于active的DAG<br><img src="https://github.com/BluceLyen/BluceLyen.github.io/blob/master/image/first_dag.png?raw=true" alt=""><br>将DAG状态更改为On状态并点击刷新，DAG就可以被airflow sceduler命令调度了，灰色长方形为调度时间<br><img src="https://github.com/BluceLyen/BluceLyen.github.io/blob/master/image/dagon_2.png?raw=true" alt=""></p>
<h4 id="5-3_测试DAG">5.3 测试DAG</h4><pre><code><span class="preprocessor">#列出所有的DAG</span>
airflow list_dags
<span class="preprocessor">#列出特定DAG下的所有task</span>
airflow list_tasks first_bash_dag
<span class="preprocessor">#测试DAG中的某个task</span>
airflow test first_bash_dag print_date <span class="number">2017</span>-<span class="number">1</span>-<span class="number">03</span>
</code></pre><h4 id="5-4_运行DAG">5.4 运行DAG</h4><p>PS:建议一个DAG中所有的task经过airflow test后再执行相应的airflow scheduler</p>
<p>1.为<code>first_bash_dag</code>开启一个特定的调度(scheduler)</p>
<p>airflow scheduler -d <code>first_bash_dag</code> -sd=/home/lyen/airflow/dags</p>
<p>PS:</p>
<p>airflow scheduler [-d DAG_ID] -sd=/home/lyen/airflow/dags  [-n NUM_RUNS]</p>
<p>启动dag调度器, 注意启动调度器,并不意味着dag会被马上触发,dag触发需要符合它自的schedule规则.参数NUM_RUNS,如果指定的话,dag将在运行NUM_RUNS次后退出.没有指定时, scheduler将一直运行</p>
<p>参数DAG_ID可以设定, 也可以缺省,含义分别是:如果设定了DAG_ID,则为该DAG_ID专门启动一个scheduler;如果缺省DAG_ID,airflow会为每个dag(subdag除外)都启动一个scheduler</p>
<p>2.DAG运行的日志保存在~/airflow/logs中,使用tree <code>first_bash_dag</code>查看目录结构</p>
<p><img src="https://github.com/BluceLyen/BluceLyen.github.io/blob/master/image/logs.png?raw=true" alt=""></p>
<ol>
<li>DAG 运行成功的web UI</li>
</ol>
<p><img src="https://github.com/BluceLyen/BluceLyen.github.io/blob/master/image/graph_view.png?raw=true" alt=""></p>
<p><img src="https://github.com/BluceLyen/BluceLyen.github.io/blob/master/image/tree_view.png?raw=true" alt=""></p>
<p>PS: web UI中其他功能就不一一赘述了，实践是检验真理的唯一标准</p>
<h4 id="5-5_关于schedule_interval">5.5 关于schedule_interval</h4><p>schedule_interval被定义为一个DAG参数,并接收一个cron表达式,或者一个datetime,timedelta对象,<a href="http://pythonhosted.org/airflow/scheduler.html?highlight=cron" target="_blank" rel="external">详见Airflow官方文档</a></p>
<p><img src="https://github.com/BluceLyen/BluceLyen.github.io/blob/master/image/interval.png?raw=ture" alt=""></p>
<h3 id="6-_HiveOperator">6. HiveOperator</h3><p>PS: 本地应有hadoop与hive环境</p>
<pre><code>from airflow import DAG
from airflow.operators.hive_operator import HiveOperator
from datetime import datetime,timedelta

default_args = {
    <span class="string">'owner'</span>: <span class="string">'lyen'</span>,
    <span class="string">'depends_on_past'</span>: <span class="literal">False</span>,
    <span class="string">'start_date'</span>: datetime(<span class="number">2016</span>,<span class="number">12</span>,<span class="number">26</span>),
    <span class="string">'email'</span>: [<span class="string">'lienlian@wolongdata.com'</span>],
    <span class="string">'email_on_failure'</span>: <span class="literal">True</span>,
    <span class="string">'email_on_retry'</span>: <span class="literal">False</span>,
    <span class="preprocessor"># <span class="string">'retries'</span>: 1,</span>
    <span class="preprocessor">#<span class="string">'retry_delay'</span>: timedelta(minutes=2),</span>
    <span class="preprocessor">#<span class="string">'end_date'</span>: datetime(2016, 12, 21),</span>
    <span class="preprocessor"># <span class="string">'queue'</span>: <span class="string">'bash_queue'</span>,</span>
    <span class="preprocessor"># <span class="string">'pool'</span>: <span class="string">'backfill'</span>,</span>
    <span class="preprocessor"># <span class="string">'priority_weight'</span>: 10,</span>
}

dag = DAG(<span class="string">'hive_operator'</span>,default_args=default_args,schedule_interval=<span class="string">'40 16 * * *'</span>)

task_hive_cmd = <span class="string">"create table lel.wc_2 as select w.word as word,count(1) as count from (select explode(split(id,' ')) as word from lel.wc_text) w group by word order by count desc"</span>
task_hive = HiveOperator(task_id=<span class="string">'hive_task'</span>,
                      hql=task_hive_cmd,
                      dag=dag)
</code></pre><h3 id="7-_SSHHook远程提交ETL数据流">7. SSHHook远程提交ETL数据流</h3><p>使用SSHExecuteOperator远程提交bash命令实现data pipline,更多Hook详见<a href="http://pythonhosted.org/airflow/code.html#module-airflow.hooks" target="_blank" rel="external">Airflow官方文档 HookS</a></p>
<h4 id="7-1_web_UI新建ssh_Connection">7.1 web UI新建ssh Connection</h4><p><img src="https://github.com/BluceLyen/BluceLyen.github.io/blob/master/image/sshconn.png?raw=true" alt=""></p>
<h4 id="7-2_ssh免密码登录">7.2 ssh免密码登录</h4><p><a href="http://chenlb.iteye.com/blog/211809" target="_blank" rel="external">配置ssh无密码远程登录</a></p>
<h4 id="7-3_airflow_python_脚本编写">7.3 airflow python 脚本编写</h4><p>场景:闲鱼数据入库，pyspark提交spark数据解析程序，hive更新迭代数据并入库</p>
<pre><code><span class="comment">#encoding:utf-8</span>
<span class="keyword">from</span> airflow <span class="keyword">import</span> DAG
<span class="keyword">from</span> airflow.operators.bash_operator <span class="keyword">import</span> BashOperator
<span class="keyword">from</span> datetime <span class="keyword">import</span> datetime,timedelta
<span class="keyword">from</span> airflow.contrib.operators.ssh_execute_operator <span class="keyword">import</span> SSHExecuteOperator
<span class="keyword">from</span> airflow.contrib.hooks.ssh_hook <span class="keyword">import</span> SSHHook
<span class="keyword">from</span> airflow.operators.email_operator <span class="keyword">import</span> EmailOperator

<span class="comment">#解决python中文字符集问题</span>
<span class="keyword">import</span> sys
reload(sys)
sys.setdefaultencoding(<span class="string">'utf-8'</span>)

default_args = {
    <span class="string">'owner'</span>: <span class="string">'airflow'</span>,
    <span class="string">'depends_on_past'</span>: <span class="keyword">False</span>,
    <span class="string">'start_date'</span>: datetime(<span class="number">2017</span>,<span class="number">1</span>,<span class="number">3</span>),
    <span class="string">'email'</span>: [<span class="string">'lienlian@wolongdata.com'</span>],
    <span class="string">'email_on_failure'</span>: <span class="keyword">True</span>,
    <span class="string">'email_on_retry'</span>: <span class="keyword">False</span>,
    <span class="comment"># 'retries': 1,</span>
    <span class="comment">#'retry_delay': timedelta(minutes=2),</span>
    <span class="comment">#'end_date': datetime(2016, 12, 21),</span>
    <span class="comment"># 'queue': 'bash_queue',</span>
    <span class="comment"># 'pool': 'backfill',</span>
    <span class="comment"># 'priority_weight': 10,</span>

}

dag = DAG(<span class="string">'xianyu_itemid_update'</span>,default_args=default_args,schedule_interval=<span class="string">'30 7 * * *'</span>)
<span class="comment">#conn_id与web UI中配置的conn_id一致</span>
sshHook = SSHHook(conn_id=<span class="string">"cs220"</span>)

hive = SSHExecuteOperator(
    task_id=<span class="string">"hive"</span>,
    bash_command=<span class="string">'(bash /home/lel/wolong/sparkdata/lel/airflow/xianyu/source/xianyu_iteminfo_import.sh)'</span>,
    ssh_hook=sshHook,
    output_encoding=<span class="string">'utf-8'</span>,
    env={<span class="string">"shell"</span>:<span class="string">"/bin/bash"</span>},
    dag=dag)

pyspark = SSHExecuteOperator(
    task_id=<span class="string">"pyspark"</span>,
    bash_command=<span class="string">'(bash /home/lel/wolong/sparkdata/lel/airflow/xianyu/source/xianyu_iteminfo_parse.sh)'</span>,
    ssh_hook=sshHook,
    output_encoding=<span class="string">'utf-8'</span>,
    env={<span class="string">"shell"</span>:<span class="string">"/bin/bash"</span>},
    dag=dag)
<span class="comment">#hive和spark任务成功后邮件通知  </span>
email = EmailOperator(task_id=<span class="string">'email'</span>,
                      to=[<span class="string">'lienlian@wolongdata.com'</span>],
                      subject=<span class="string">'airflow schedule hive and spark task'</span>,
                      html_content=<span class="string">'successfully execute the tasks'</span>,
                      dag=dag)
pyspark.set_downstream(hive)
hive.set_downstream(email)
</code></pre><h4 id="7-4_相应的bash文件内容">7.4 相应的bash文件内容</h4><p><code>xianyu_iteminfo_parse.sh</code></p>
<pre><code>#!/bin/bash
<span class="keyword">source</span> ~/.bashrc
<span class="keyword">date</span>
<span class="keyword">date</span>  +<span class="variable">%Y</span><span class="variable">%m</span><span class="variable">%d</span>
lastday=<span class="variable">$(</span><span class="keyword">date</span> -d <span class="string">'1 days ago'</span> +<span class="variable">%Y</span><span class="variable">%m</span><span class="variable">%d</span>)
thedaybeforelastday=<span class="variable">$(</span><span class="keyword">date</span> -d <span class="string">'2 days ago'</span> +<span class="variable">%Y</span><span class="variable">%m</span><span class="variable">%d</span>)

hadoop_env=/home/lel/hadoop/bin
spark_env=/home/lel/spark/bin
work_place=/home/lel/wolong/sparkdata/lel

<span class="variable">$hadoop_env</span>/hadoop fs -test -e /user/lel/temp/xianyu_comment_2016
<span class="keyword">if</span> [ <span class="variable">$?</span> -eq <span class="number">0</span> ] ;then
<span class="variable">$hadoop_env</span>/hadoop fs  -rmr /user/lel/temp/xianyu_comment_2016
<span class="keyword">else</span>
echo <span class="string">'Directory is not exist,you can run you spark job as you want!!!'</span>
fi

<span class="variable">$spark_env</span>/spark-submit  --executor-<span class="keyword">memory</span> <span class="number">6</span>G  --driver-<span class="keyword">memory</span> <span class="number">6</span>G  --total-executor-cores <span class="number">80</span> <span class="variable">$work_place</span>/spark/xianyu/t_xianyu.py <span class="variable">$lastday</span>
</code></pre><p><code>xianyu_iteminfo_import.sh</code></p>
<pre><code>#!/usr/bin

source ~/.bashrc
date
date  +%Y%m%d
lastday=$(date -d '<span class="number">1</span> days ago' +%Y%m%d)
thedaybeforelastday=$(date -d '<span class="number">2</span> days ago' +%Y%m%d)

hive_env=/home/lel/hive/bin
table=wlbase_dev.t_base_ec_xianyu_iteminfo

$hive_env/hive&lt;&lt;EOF
<span class="keyword">use</span> wlbase_dev;
LOAD DATA  INPATH '/user/lel/temp/xianyu_2016' OVERWRITE INTO TABLE $table PARTITION (ds=<span class="attribute">'tmp</span>');
insert OVERWRITE table wlbase_dev.t_base_ec_xianyu_iteminfo PARTITION(ds = $lastday)
<span class="keyword">select</span>
<span class="keyword">case</span> <span class="keyword">when</span> t1.itemid <span class="keyword">is</span> <span class="keyword">null</span> <span class="keyword">then</span> t2.itemid <span class="keyword">else</span> t1.itemid <span class="keyword">end</span>,
<span class="keyword">case</span> <span class="keyword">when</span> t1.itemid <span class="keyword">is</span> <span class="keyword">null</span> <span class="keyword">then</span> t2.userid <span class="keyword">else</span> t1.userid <span class="keyword">end</span>,
<span class="keyword">case</span> <span class="keyword">when</span> t1.itemid <span class="keyword">is</span> <span class="keyword">null</span> <span class="keyword">then</span> t2.phone <span class="keyword">else</span> t1.phone <span class="keyword">end</span>,
<span class="keyword">case</span> <span class="keyword">when</span> t1.itemid <span class="keyword">is</span> <span class="keyword">null</span> <span class="keyword">then</span> t2.contacts <span class="keyword">else</span> t1.contacts <span class="keyword">end</span>,
<span class="keyword">case</span> <span class="keyword">when</span> t1.itemid <span class="keyword">is</span> <span class="keyword">null</span> <span class="keyword">then</span> t2.title <span class="keyword">else</span> t1.title <span class="keyword">end</span>,
<span class="keyword">case</span> <span class="keyword">when</span> t1.itemid <span class="keyword">is</span> <span class="keyword">null</span> <span class="keyword">then</span> t2.province <span class="keyword">else</span> t1.province <span class="keyword">end</span>,
<span class="keyword">case</span> <span class="keyword">when</span> t1.itemid <span class="keyword">is</span> <span class="keyword">null</span> <span class="keyword">then</span> t2.city <span class="keyword">else</span> t1.city <span class="keyword">end</span>,
<span class="keyword">case</span> <span class="keyword">when</span> t1.itemid <span class="keyword">is</span> <span class="keyword">null</span> <span class="keyword">then</span> t2.area <span class="keyword">else</span> t1.area <span class="keyword">end</span>,
<span class="keyword">case</span> <span class="keyword">when</span> t1.itemid <span class="keyword">is</span> <span class="keyword">null</span> <span class="keyword">then</span> t2.auctionType <span class="keyword">else</span> t1.auctionType <span class="keyword">end</span>,
<span class="keyword">case</span> <span class="keyword">when</span> t1.itemid <span class="keyword">is</span> <span class="keyword">null</span> <span class="keyword">then</span> t2.description <span class="keyword">else</span> t1.description <span class="keyword">end</span>,
<span class="keyword">case</span> <span class="keyword">when</span> t1.itemid <span class="keyword">is</span> <span class="keyword">null</span> <span class="keyword">then</span> t2.detailFrom <span class="keyword">else</span> t1.detailFrom <span class="keyword">end</span>,
<span class="keyword">case</span> <span class="keyword">when</span> t1.itemid <span class="keyword">is</span> <span class="keyword">null</span> <span class="keyword">then</span> t2.favorNum <span class="keyword">else</span> t1.favorNum <span class="keyword">end</span>,
<span class="keyword">case</span> <span class="keyword">when</span> t1.itemid <span class="keyword">is</span> <span class="keyword">null</span> <span class="keyword">then</span> t2.commentNum <span class="keyword">else</span> t1.commentNum <span class="keyword">end</span>,
<span class="keyword">case</span> <span class="keyword">when</span> t1.itemid <span class="keyword">is</span> <span class="keyword">null</span> <span class="keyword">then</span> t2.firstModified <span class="keyword">else</span> t1.firstModified <span class="keyword">end</span>,
<span class="keyword">case</span> <span class="keyword">when</span> t1.itemid <span class="keyword">is</span> <span class="keyword">null</span> <span class="keyword">then</span> t2.firstModifiedDiff <span class="keyword">else</span> t1.firstModifiedDiff <span class="keyword">end</span>,
<span class="keyword">case</span> <span class="keyword">when</span> t1.itemid <span class="keyword">is</span> <span class="keyword">null</span> <span class="keyword">then</span> t2.t_from <span class="keyword">else</span> t1.t_from <span class="keyword">end</span>,
<span class="keyword">case</span> <span class="keyword">when</span> t1.itemid <span class="keyword">is</span> <span class="keyword">null</span> <span class="keyword">then</span> t2.gps <span class="keyword">else</span> t1.gps <span class="keyword">end</span>,
<span class="keyword">case</span> <span class="keyword">when</span> t1.itemid <span class="keyword">is</span> <span class="keyword">null</span> <span class="keyword">then</span> t2.offline <span class="keyword">else</span> t1.offline <span class="keyword">end</span>,
<span class="keyword">case</span> <span class="keyword">when</span> t1.itemid <span class="keyword">is</span> <span class="keyword">null</span> <span class="keyword">then</span> t2.originalPrice <span class="keyword">else</span> t1.originalPrice <span class="keyword">end</span>,
<span class="keyword">case</span> <span class="keyword">when</span> t1.itemid <span class="keyword">is</span> <span class="keyword">null</span> <span class="keyword">then</span> t2.price <span class="keyword">else</span> t1.price <span class="keyword">end</span>,
<span class="keyword">case</span> <span class="keyword">when</span> t1.itemid <span class="keyword">is</span> <span class="keyword">null</span> <span class="keyword">then</span> t2.postprice <span class="keyword">else</span> t1.postprice <span class="keyword">end</span>,
<span class="keyword">case</span> <span class="keyword">when</span> t1.itemid <span class="keyword">is</span> <span class="keyword">null</span> <span class="keyword">then</span> t2.userNick <span class="keyword">else</span> t1.userNick <span class="keyword">end</span>,
<span class="keyword">case</span> <span class="keyword">when</span> t1.itemid <span class="keyword">is</span> <span class="keyword">null</span> <span class="keyword">then</span> t2.categoryid <span class="keyword">else</span> t1.categoryid <span class="keyword">end</span>,
<span class="keyword">case</span> <span class="keyword">when</span> t1.itemid <span class="keyword">is</span> <span class="keyword">null</span> <span class="keyword">then</span> t2.categoryName <span class="keyword">else</span> t1.categoryName <span class="keyword">end</span>,
<span class="keyword">case</span> <span class="keyword">when</span> t1.itemid <span class="keyword">is</span> <span class="keyword">null</span> <span class="keyword">then</span> t2.fishPoolid <span class="keyword">else</span> t1.fishPoolid <span class="keyword">end</span>,
<span class="keyword">case</span> <span class="keyword">when</span> t1.itemid <span class="keyword">is</span> <span class="keyword">null</span> <span class="keyword">then</span> t2.fishpoolName <span class="keyword">else</span> t1.fishpoolName <span class="keyword">end</span>,
<span class="keyword">case</span> <span class="keyword">when</span> t1.itemid <span class="keyword">is</span> <span class="keyword">null</span> <span class="keyword">then</span> t2.bar <span class="keyword">else</span> t1.bar <span class="keyword">end</span>,
<span class="keyword">case</span> <span class="keyword">when</span> t1.itemid <span class="keyword">is</span> <span class="keyword">null</span> <span class="keyword">then</span> t2.barInfo <span class="keyword">else</span> t1.barInfo <span class="keyword">end</span>,
<span class="keyword">case</span> <span class="keyword">when</span> t1.itemid <span class="keyword">is</span> <span class="keyword">null</span> <span class="keyword">then</span> t2.abbr <span class="keyword">else</span> t1.abbr <span class="keyword">end</span>,
<span class="keyword">case</span> <span class="keyword">when</span> t1.itemid <span class="keyword">is</span> <span class="keyword">null</span> <span class="keyword">then</span> t2.zhima <span class="keyword">else</span> t1.zhima <span class="keyword">end</span>,
<span class="keyword">case</span> <span class="keyword">when</span> t1.itemid <span class="keyword">is</span> <span class="keyword">null</span> <span class="keyword">then</span> t2.shiren <span class="keyword">else</span> t1.shiren <span class="keyword">end</span>,
<span class="keyword">case</span> <span class="keyword">when</span> t1.itemid <span class="keyword">is</span> <span class="keyword">null</span> <span class="keyword">then</span> t2.ts <span class="keyword">else</span> t1.ts <span class="keyword">end</span>
from
(<span class="keyword">select</span> * from  $table where ds = <span class="attribute">'tmp</span>')t1
full outer JOIN
(<span class="keyword">select</span> * from $table where ds = $thedaybeforelastday)t2
<span class="keyword">ON</span>
t1.itemid = t2.itemid;
EOF
</code></pre><h4 id="7-5_SSHExecuteOperator注意事项">7.5 SSHExecuteOperator注意事项</h4><ol>
<li>使用SSHHook远程提交bash脚本运行hive或者spark的时候注意一定要使用绝对路径,比如:<br>/home/lel/hive/bin/hive ~/test.hql而不能直接使用hive ~/test.hql</li>
</ol>
<h3 id="Q&amp;A">Q&amp;A</h3><p>1.频繁修改同一个airflow python 脚本，airflow scheduler调度不起作用</p>
<p>此问题属于airflow暂时的不足之处，解决办法是编辑脚本赋予DAG一个新的dag_id，然后初始化数据库，重新执行airflow scheduler，所以在测试一个DAG是否正常执行时建议首先考虑对DAG中的每个task进行airflow test …，倘若问题依然无法解决，再考虑kill airflow 的webservewr服务器进程再重启airflow webserver</p>
<p>2.python中文字符集报错</p>
<p>只需要在代码代码中加入如下几行代码：</p>
<pre><code>import sys
<span class="function"><span class="title">reload</span><span class="params">(sys)</span></span>
sys.<span class="function"><span class="title">setdefaultencoding</span><span class="params">(<span class="string">'utf-8'</span>)</span></span>
</code></pre>
      
    </div>
    <footer>
      
          
<!-- JiaThis Button BEGIN -->
<div class="jiathis_style">
  <span class="jiathis_txt">分享到：</span>
  <a class="jiathis_button_weixin">微信</a>
  <a class="jiathis_button_tsina">新浪微博</a>
  <a class="jiathis_button_renren">人人网</a>
  <a class="jiathis_button_qzone">QQ空间</a>
  <a class="jiathis_button_douban">豆瓣</a>
  <a class="jiathis_button_pocket">Pocket</a>
  <a href="http://www.jiathis.com/share?uid=901656" class="jiathis jiathis_txt jiathis_separator jtico jtico_jiathis" target="_blank">更多</a>
  <a class="jiathis_counter_style"></a>
</div>
<script type="text/javascript" src="http://v3.jiathis.com/code_mini/jia.js?uid=901656" charset="utf-8"></script>
<!-- JiaThis Button END -->

          <div class="clearfix"></div>
          <nav id="pagination">
  
  
    <a href="/2016/12/02/TriangleCount/" class="alignright next">Prev<i class="fa fa-long-arrow-right"></i></a>
  
  <div class="clearfix"></div>
</nav>
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>


<section id="comment">
  <h1 class="title">Comentarios</h1>

  
      <!-- Duoshuo Comment BEGIN -->
<div class="ds-thread" data-thread-key="/2017/01/04/airflow_useage/"></div>
<!-- Duoshuo Comment END -->
  
</section>



    <div class="clearfix"></div>
  </div>
  <footer id="footer" class="inner"><div>
  
  &copy; 2017 Lyen
  
</div>
Powered by <a href="http://zespia.tw/hexo/" title="Hexo" target="_blank" rel="external">Hexo</a> and <a href="http://pages.github.com/" title="GitHub Pages" target="_blank" rel="external">GitHub Pages</a>

<div class="clearfix"></div></footer>
  
<script src="/js/jquery.imagesloaded.min.js" type="text/javascript"></script>
<script src="/js/gallery.js" type="text/javascript"></script>
<script src="//netdna.bootstrapcdn.com/bootstrap/3.1.0/js/bootstrap.min.js" type="text/javascript"></script>




    <script type="text/javascript">
        (function(){

            $(window).scroll(function(){

                var scrollTop = $(window).scrollTop();
                if ( scrollTop >200 ){
                    $("#main-nav").removeClass('normal_mode').addClass('top_mode');
                } else{
                    $("#main-nav").removeClass('top_mode').addClass('normal_mode');
                }

            });

        })();
    </script>



  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css" media="screen" type="text/css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>
  <script type="text/javascript">
  (function($){
    $('.fancybox').fancybox({
      'titlePosition': 'inside'
    });
  })(jQuery);
  </script>



    
    <script type="text/javascript">
      var duoshuoQuery = {short_name:"BLuceLyen"};
    (function() {
      var ds = document.createElement('script');
      ds.type = 'text/javascript';ds.async = true;
      ds.src = 'http://static.duoshuo.com/embed.js';
      ds.charset = 'UTF-8';
      (document.getElementsByTagName('head')[0] 
      || document.getElementsByTagName('body')[0]).appendChild(ds);
    })();
  </script>



<script type="text/javascript">
  
  $(function(){

    $('.title').hover(
      function() {      
        $(this).stop().animate(
          {'marginLeft': '10px'}, 200
        );   
      }, 
      function() {       
        $(this).stop().animate({'marginLeft': '0px'}, 200);      
      
    });   

  });

</script>


</body>
</html>